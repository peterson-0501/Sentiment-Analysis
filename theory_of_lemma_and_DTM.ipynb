{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f22d1-ce5f-4e8d-9a34-52fadc8bf72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance of catego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7abba0-a21d-43b0-aad8-d9c519b3c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning for sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2d796-327b-444a-85e2-2f3b55c4b6dd",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262744d7-b911-49d4-b3f7-0ab6480bd1bf",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing a word to its base or root form, which is known as its lemma. This process involves understanding the context in which a word is used and ensuring that the lemma corresponds to a valid word in the language. The algorithm used for lemmatization typically relies on several components and can vary depending on the specific implementation. However, the most common and widely used lemmatization algorithms include:\n",
    "\n",
    "### 1. **Rule-Based Lemmatization**\n",
    "Rule-based lemmatizers use a set of manually crafted rules to identify the base form of a word. These rules often include:\n",
    "- Morphological analysis to identify suffixes and prefixes.\n",
    "- A dictionary or lexicon to look up the lemma of a word.\n",
    "- Part-of-speech tagging to ensure the correct lemma is chosen based on the word's grammatical role.\n",
    "\n",
    "### 2. **Dictionary-Based Lemmatization**\n",
    "This approach relies heavily on a pre-compiled dictionary or lexicon that maps inflected forms of words to their base forms. The algorithm:\n",
    "- Looks up the word in the dictionary.\n",
    "- Identifies its lemma based on the dictionary entry.\n",
    "\n",
    "### 3. **Statistical or Machine Learning-Based Lemmatization**\n",
    "In more advanced approaches, statistical models or machine learning techniques are used. These models are trained on large corpora of text and learn to predict the base form of a word based on its context. Components include:\n",
    "- Training data consisting of annotated text where each word is labeled with its lemma.\n",
    "- A statistical model (e.g., Hidden Markov Model, Conditional Random Field) or a machine learning model (e.g., neural networks).\n",
    "- Features such as the word itself, its context, and part-of-speech tags.\n",
    "\n",
    "### Common Lemmatization Tools and Libraries\n",
    "Several libraries and tools implement these algorithms. Notable examples include:\n",
    "\n",
    "1. **WordNet Lemmatizer (NLTK)**: This is part of the Natural Language Toolkit (NLTK) and relies on the WordNet lexical database to find lemmas.\n",
    "2. **spaCy**: An industrial-strength NLP library that provides a high-performance lemmatizer as part of its pipeline.\n",
    "3. **Stanford CoreNLP**: A comprehensive NLP toolkit that includes a lemmatizer with support for multiple languages.\n",
    "4. **TextBlob**: A simple NLP library built on top of NLTK and Pattern that includes lemmatization.\n",
    "\n",
    "### Example of Rule-Based Lemmatization\n",
    "Here's a simple example of how a rule-based lemmatizer might work for the word \"running\":\n",
    "\n",
    "1. **Input Word**: running\n",
    "2. **Morphological Analysis**: Identify suffix \"-ing\".\n",
    "3. **Part-of-Speech Tagging**: Determine that \"running\" is a verb.\n",
    "4. **Apply Rules**: Remove the suffix \"-ing\" and check for the base form \"run\".\n",
    "5. **Output Lemma**: run\n",
    "\n",
    "### Example of Dictionary-Based Lemmatization\n",
    "Using a dictionary, the process might be:\n",
    "\n",
    "1. **Input Word**: ran\n",
    "2. **Dictionary Lookup**: Find \"ran\" in the dictionary with an entry pointing to \"run\".\n",
    "3. **Output Lemma**: run\n",
    "\n",
    "### Example of Statistical Lemmatization\n",
    "For a machine learning-based approach:\n",
    "\n",
    "1. **Input Word**: running\n",
    "2. **Contextual Analysis**: Consider the surrounding words and part-of-speech tags.\n",
    "3. **Model Prediction**: Use the trained model to predict the lemma \"run\".\n",
    "4. **Output Lemma**: run\n",
    "\n",
    "In summary, the specific \"leema\" or lemma-finding process used in lemmatization depends on the chosen algorithm and the implementation details of the tool or library being used. Whether rule-based, dictionary-based, or statistical, each approach has its strengths and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1024de-084e-49c9-ae5c-f88068d296e6",
   "metadata": {},
   "source": [
    "## Algorithm of spaCy and NLKT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ebdb7-f186-4562-ac4c-c6437128dad6",
   "metadata": {},
   "source": [
    "\n",
    "### spaCy\n",
    "spaCy uses a combination of rule-based and dictionary-based methods, tightly integrated with part-of-speech tagging. Here is a step-by-step outline of the algorithm:\n",
    "\n",
    "1. **Part-of-Speech Tagging**: First, spaCy assigns a part-of-speech tag to each word in the text. This helps understand the grammatical role of the word (e.g., noun, verb, adjective).\n",
    "\n",
    "2. **Rule-Based Morphological Analysis**: spaCy applies a set of predefined rules to transform the word into its base form. These rules cover common morphological changes, such as:\n",
    "   - Removing suffixes like \"-ing\", \"-ed\", \"-s\".\n",
    "   - Handling regular forms of verbs and nouns.\n",
    "\n",
    "3. **Lookup in Lexical Resources**: spaCy uses its internal lexical database (word lists) to find the lemma. This database includes mappings from various word forms to their base forms. For example, it maps irregular forms like \"went\" to \"go\".\n",
    "\n",
    "4. **Combining Rules and Lexical Resources**: The algorithm combines the results from the rule-based transformations and lexical lookup to determine the most accurate lemma.\n",
    "\n",
    "5. **Output Lemma**: Finally, spaCy outputs the lemma for each word based on the applied rules and dictionary lookup.\n",
    "\n",
    "### NLTK (WordNet Lemmatizer)\n",
    "NLTK's WordNet Lemmatizer relies primarily on the WordNet lexical database. Here's a detailed breakdown of the algorithm:\n",
    "\n",
    "1. **Part-of-Speech Tagging**: The WordNet lemmatizer requires the part-of-speech tag for each word to select the correct lemma. Users must provide this tag or use an external POS tagger.\n",
    "\n",
    "2. **WordNet Lookup**: The lemmatizer looks up the word in the WordNet database using the provided part-of-speech tag. WordNet contains extensive mappings of words to their base forms.\n",
    "\n",
    "3. **Simple Rule-Based Transformations**: For words not found in WordNet or for regular transformations, the lemmatizer applies basic morphological rules, such as:\n",
    "   - Removing common suffixes.\n",
    "   - Handling plurals and regular verb forms.\n",
    "\n",
    "4. **Fallback Mechanism**: If the word is not found in the dictionary and cannot be transformed using rules, the algorithm may return the word itself as the lemma.\n",
    "\n",
    "5. **Output Lemma**: The algorithm outputs the lemma based on the dictionary lookup or the rule-based transformation.\n",
    "\n",
    "### Example: spaCy Algorithm in Code\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"running runs ran\")\n",
    "\n",
    "# Print the base forms (lemmas) of the words\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)  # Output: ['run', 'run', 'run']\n",
    "```\n",
    "In this example, spaCy:\n",
    "1. Tags \"running\", \"runs\", and \"ran\" as verbs.\n",
    "2. Applies rules to remove \"-ing\" and \"-s\".\n",
    "3. Uses its internal dictionary to map \"ran\" to \"run\".\n",
    "4. Outputs the lemma \"run\" for all forms.\n",
    "\n",
    "### Example: NLTK Algorithm in Code\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Create the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words with part-of-speech tags\n",
    "lemmas = [\n",
    "    lemmatizer.lemmatize(\"running\", pos=wordnet.VERB),\n",
    "    lemmatizer.lemmatize(\"runs\", pos=wordnet.VERB),\n",
    "    lemmatizer.lemmatize(\"ran\", pos=wordnet.VERB)\n",
    "]\n",
    "print(lemmas)  # Output: ['run', 'run', 'run']\n",
    "```\n",
    "In this example, NLTK:\n",
    "1. Receives the part-of-speech tags for \"running\", \"runs\", and \"ran\" as verbs.\n",
    "2. Looks up each word in the WordNet database.\n",
    "3. Applies simple rules if necessary.\n",
    "4. Outputs the lemma \"run\" for all forms.\n",
    "\n",
    "### Summary\n",
    "- **spaCy** uses a combination of rules and a dictionary, guided by part-of-speech tags, to find the lemma.\n",
    "- **NLTK** primarily relies on the WordNet dictionary, supplemented by simple rules and part-of-speech tags, to determine the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c0b2-c848-4fe9-a7fa-dc2f0989f840",
   "metadata": {},
   "source": [
    "# Comparison of stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3d8af-30c1-4d02-8f2d-7aa999955cad",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base or root forms, but they differ significantly in their approaches and outcomes. Here’s a detailed comparison:\n",
    "\n",
    "### Stemming\n",
    "**Definition**: Stemming is the process of reducing a word to its base or root form, usually by removing suffixes. The root form is not necessarily a valid word in the language.\n",
    "\n",
    "**Algorithm**: Stemming algorithms, like the Porter Stemmer, use a set of simple, rule-based transformations. These rules are designed to strip common suffixes such as \"ing\", \"ly\", \"es\", \"s\", etc.\n",
    "\n",
    "**Example**:\n",
    "- \"running\" -> \"run\"\n",
    "- \"happily\" -> \"happi\"\n",
    "- \"studies\" -> \"studi\"\n",
    "\n",
    "**Advantages**:\n",
    "- **Speed**: Stemming is usually faster because it applies simple rules.\n",
    "- **Simplicity**: The algorithms are relatively straightforward and easy to implement.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Accuracy**: The resulting root form may not be a valid word. For example, \"happily\" becomes \"happi\".\n",
    "- **Over-stemming and Under-stemming**: Stemming can sometimes remove too much (over-stemming) or too little (under-stemming), leading to incorrect root forms.\n",
    "\n",
    "### Lemmatization\n",
    "**Definition**: Lemmatization is the process of reducing a word to its base or dictionary form (lemma), ensuring that the base form is a valid word. It considers the context and part of speech of the word.\n",
    "\n",
    "**Algorithm**: Lemmatization algorithms use complex rules and dictionaries. They often rely on part-of-speech tagging to choose the correct base form.\n",
    "\n",
    "**Example**:\n",
    "- \"running\" -> \"run\"\n",
    "- \"happily\" -> \"happy\"\n",
    "- \"studies\" -> \"study\" (if noun) or \"study\" (if verb)\n",
    "\n",
    "**Advantages**:\n",
    "- **Accuracy**: The result is always a valid word, making lemmatization more accurate.\n",
    "- **Context Awareness**: Lemmatization considers the context and grammatical role, leading to better results for words that have multiple forms.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Speed**: Lemmatization is usually slower because it involves more complex processing, including part-of-speech tagging.\n",
    "- **Complexity**: The algorithms are more complex and require more resources (like lexical databases).\n",
    "\n",
    "### Which is Better?\n",
    "**Use Stemming When**:\n",
    "- You need a fast and simple solution.\n",
    "- You’re working with applications where perfect accuracy is not critical (e.g., search engines, where rough matches are acceptable).\n",
    "- Computational resources are limited.\n",
    "\n",
    "**Use Lemmatization When**:\n",
    "- You need higher accuracy and the base form must be a valid word.\n",
    "- The context and grammatical structure of the words are important (e.g., text analysis, language translation, and advanced NLP tasks).\n",
    "- You have sufficient computational resources and time.\n",
    "\n",
    "### Practical Examples\n",
    "#### Stemming with NLTK:\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"happily\", \"studies\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)  # Output: ['run', 'happili', 'studi']\n",
    "```\n",
    "\n",
    "#### Lemmatization with NLTK:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"happily\", \"studies\"]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=wordnet.VERB) if word == \"studies\" else lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmas)  # Output: ['run', 'happy', 'study']\n",
    "```\n",
    "\n",
    "#### Lemmatization with spaCy:\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"running happily studies\")\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)  # Output: ['run', 'happy', 'study']\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "- **Stemming** is faster and simpler but less accurate.\n",
    "- **Lemmatization** is more accurate and context-aware but slower and more complex.\n",
    "\n",
    "Choosing between stemming and lemmatization depends on the specific requirements of your NLP task, balancing the trade-offs between speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e977c-9544-4862-801f-e4b1e7295950",
   "metadata": {},
   "source": [
    "# Different ways to create a DTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76dd5f-e7f8-4034-810f-7a7134a9d3c0",
   "metadata": {},
   "source": [
    "## 1. Binary DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d61df0-5a1a-4444-94c4-207a1d98a327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       1    1      0     1\n",
       "2    1         0      0   1    1       0    1      1     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "binary_dtm = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "binary_dtm =  pd.DataFrame(binary_dtm.toarray(), columns=terms)\n",
    "binary_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec35f1-ce6e-4de5-8bf8-55d8fba7e53e",
   "metadata": {},
   "source": [
    "## 2. Frequency DTM (Bags of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d39a59c-2085-4c66-a22f-8c1e47857248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         2      0   1    0       1    1      0     1\n",
       "2    1         0      0   1    1       0    1      1     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "freq_dtm = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "freq_dtm =  pd.DataFrame(freq_dtm.toarray(), columns=terms)\n",
    "freq_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193df514-06fb-48e9-964b-befdf2fc0714",
   "metadata": {},
   "source": [
    "## 3. TF-IDF DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066752a6-7a82-49ec-9f7b-c18839c4ef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.835592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.549351</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document     first      one    second    third\n",
       "0  0.00000  0.605349  0.795961  0.00000  0.000000  0.00000\n",
       "1  0.00000  0.835592  0.000000  0.00000  0.549351  0.00000\n",
       "2  0.57735  0.000000  0.000000  0.57735  0.000000  0.57735"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"first document\", \"document second document\", \"And third one\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_dtm =  pd.DataFrame(tfidf_dtm.toarray(), columns=terms)\n",
    "tfidf_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc2511-3011-4bdc-a0e5-fdc29d15858e",
   "metadata": {},
   "source": [
    "\n",
    "1. **Term Frequency (TF)**: This measures how often a term (word) appears in a document. It is calculated as the ratio of the count of the term to the total number of terms in the document. It helps to give more weight to terms that occur more frequently in the document.\n",
    "\n",
    "   $$ TF_{t,d} = \\frac{{\\text{Number of times term } t \\text{ appears in document } d}}{{\\text{Total number of terms in document } d}} $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: This measures how important a term is within the entire corpus. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. This helps to penalize words that occur too frequently across all documents.\n",
    "\n",
    " $$ IDF_{t} = \\log\\left(\\frac{{\\text{Total number of documents}}}{{\\text{Number of documents containing term } t}}\\right) $$\n",
    "\n",
    "Combining TF and IDF, we get the TF-IDF score for a term $ t$ in a document $ d$:\n",
    "\n",
    "$$ \\text{TF-IDF}_{t,d} = TF_{t,d} \\times IDF_{t} $$\n",
    "\n",
    "The manual calculation will yeid different result as scikit-learn uses different method for normalization and prescision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe2847-947a-4e29-8dfb-d9d130ffe588",
   "metadata": {},
   "source": [
    "The significance of multiplying the Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "   - **Purpose**: Measures the frequency of a term in a document.\n",
    "   - **Significance**: It helps to identify terms that are important within a specific document. A higher TF value indicates that the term is more significant within that particular document.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   - **Purpose**: Measures the importance of a term across the entire document collection.\n",
    "   - **Significance**: It helps to identify terms that are unique to fewer documents and therefore more significant in distinguishing one document from another. A higher IDF value indicates that the term is less common across all documents, making it more useful for distinguishing between documents.\n",
    "\n",
    "### Why Multiply TF and IDF?\n",
    "\n",
    "When you multiply TF and IDF, you achieve the following:\n",
    "\n",
    "1. **Adjust for Term Frequency**:\n",
    "   - Terms that occur frequently within a document get a higher score from the TF component. This highlights their importance within that specific document.\n",
    "\n",
    "2. **Adjust for Common Terms**:\n",
    "   - The IDF component down-weights terms that are common across many documents. This prevents common terms (e.g., \"the\", \"is\", \"and\") from being considered too important, even if they appear frequently in a document.\n",
    "\n",
    "### Combined Effect: TF-IDF\n",
    "\n",
    "The combined TF-IDF score effectively balances these two aspects:\n",
    "\n",
    "$$ \\text{TF-IDF}_{t,d} = TF_{t,d} \\times IDF_{t} $$\n",
    "\n",
    "- **High TF, High IDF**: A term that appears frequently in a document but not in many other documents will have a high TF-IDF score, indicating it's very relevant to that document.\n",
    "- **High TF, Low IDF**: A term that appears frequently in a document but also in many other documents will have a lower TF-IDF score, indicating it's less distinctive.\n",
    "- **Low TF, High IDF**: A term that appears infrequently in a document but also infrequently in other documents will have a moderate TF-IDF score.\n",
    "- **Low TF, Low IDF**: A term that appears infrequently in a document and frequently in other documents will have a low TF-IDF score.\n",
    "\n",
    "\n",
    "### Used for:\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is commonly used in natural language processing and information retrieval tasks. Here's when you might want to use TF-IDF:\n",
    "\n",
    "### 1. Text Classification:\n",
    "- **Use Case**: When you have a set of documents and you want to classify them into different categories or topics.\n",
    "- **Why TF-IDF**: TF-IDF helps in identifying the most relevant words or features that differentiate one category from another. It does this by giving higher weights to terms that are frequent in a document but rare across the entire corpus, thus capturing the discriminative power of words.\n",
    "\n",
    "### 2. Information Retrieval:\n",
    "- **Use Case**: When you want to retrieve relevant documents from a large collection based on a user's query.\n",
    "- **Why TF-IDF**: TF-IDF helps in ranking documents based on their relevance to the query. Documents containing terms that match the query but are rare in the overall corpus receive higher scores, indicating higher relevance.\n",
    "\n",
    "### 3. Search Engine Optimization (SEO):\n",
    "- **Use Case**: When optimizing web content to improve its visibility in search engine results.\n",
    "- **Why TF-IDF**: TF-IDF can help in identifying and incorporating relevant keywords into web content. By understanding the importance of words in a document relative to the entire corpus, SEO practitioners can create content that is more likely to rank well in search engine results pages.\n",
    "\n",
    "### 4. Text Summarization:\n",
    "- **Use Case**: When generating concise summaries of long documents.\n",
    "- **Why TF-IDF**: TF-IDF can be used to identify the most important sentences or phrases in a document based on the frequency of important terms. This helps in extracting key information and generating informative summaries.\n",
    "\n",
    "### 5. Text Mining and Sentiment Analysis:\n",
    "- **Use Case**: When analyzing large volumes of text data to extract insights or sentiments.\n",
    "- **Why TF-IDF**: TF-IDF can be used to identify important terms or phrases in the text data, which are then used as features for further analysis. By focusing on terms that are both frequent in a document and rare across the entire corpus, TF-IDF helps in identifying significant patterns and sentiments.\n",
    "\n",
    "In summary, TF-IDF is particularly useful in tasks where the goal is to identify important words or features in a document collection, and where the relative importance of terms needs to be considered. It's widely used in various applications across natural language processing, information retrieval, and text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cf9ec-5287-4261-a725-5187ab8d5822",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Normalized Term Frequency DTM\n",
    "\n",
    "Each entry is the frequency of a term in a document normalized by the document length (total number of terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7ed199d-eca8-4326-9a10-2d7a15a5fd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document  first        is       one    second       the  \\\n",
       "0  0.000000  0.200000    0.2  0.200000  0.000000  0.000000  0.200000   \n",
       "1  0.000000  0.333333    0.0  0.166667  0.000000  0.166667  0.166667   \n",
       "2  0.166667  0.000000    0.0  0.166667  0.166667  0.000000  0.166667   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.200000  \n",
       "1  0.000000  0.166667  \n",
       "2  0.166667  0.166667  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='l1') # norm can be l2 (euclidian) and max\n",
    "normalized_tf_dtm = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "normalized_tf_dtm =  pd.DataFrame(normalized_tf_dtm.toarray(), columns=terms)\n",
    "normalized_tf_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63fc05-5183-482b-92a2-9ca0ca6d4251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### L1 Normalization\n",
    "\n",
    "L1 normalization, also known as L1 norm or Manhattan norm, is a normalization technique that ensures the sum of the absolute values of the vector elements equals 1. In the context of a Document-Term Matrix (DTM), L1 normalization adjusts the term frequencies such that the sum of the normalized term frequencies for each document equals 1. This is useful for comparing documents of different lengths on a common scale.\n",
    "\n",
    "The formula for L1 normalization for a given term $ t $ in a document $ d $ is:\n",
    "\n",
    "$ \\text{Normalized TF}_{L1}(t, d) = \\frac{\\text{Frequency of } t \\text{ in } d}{\\sum_{t' \\in d} \\text{Frequency of } t' \\text{ in } d} $\n",
    "\n",
    "### Example of L1 Normalization\n",
    "\n",
    "Let's use scikit-learn's `TfidfVectorizer` with `use_idf=False` and `norm='l1'` to create a DTM with L1 normalization.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\"\n",
    "]\n",
    "\n",
    "# Initialize the vectorizer with L1 normalization\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "\n",
    "# Fit and transform the corpus to create the normalized term frequency DTM\n",
    "normalized_tf_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names (terms)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the DTM to an array and print it\n",
    "normalized_tf_array = normalized_tf_dtm.toarray()\n",
    "\n",
    "print(\"Terms:\", terms)\n",
    "print(\"Normalized Term Frequency DTM (L1):\\n\", normalized_tf_array)\n",
    "```\n",
    "\n",
    "### Other Normalization Techniques\n",
    "\n",
    "Besides L1 normalization, there are other normalization methods, such as L2 normalization and max normalization. Each has its specific use cases and advantages.\n",
    "\n",
    "### L2 Normalization\n",
    "\n",
    "L2 normalization, also known as L2 norm or Euclidean norm, adjusts the term frequencies such that the sum of the squares of the vector elements equals 1. This normalization technique is often used when the magnitude of the vectors is important.\n",
    "\n",
    "The formula for L2 normalization for a given term $ t $ in a document $ d $ is:\n",
    "\n",
    "$ \\text{Normalized TF}_{L2}(t, d) = \\frac{\\text{Frequency of } t \\text{ in } d}{\\sqrt{\\sum_{t' \\in d} (\\text{Frequency of } t' \\text{ in } d)^2}} $\n",
    "\n",
    "### Example of L2 Normalization\n",
    "\n",
    "Let's use scikit-learn's `TfidfVectorizer` with `use_idf=False` and `norm='l2'` to create a DTM with L2 normalization.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer with L2 normalization\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "\n",
    "# Fit and transform the corpus to create the normalized term frequency DTM\n",
    "normalized_tf_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the DTM to an array and print it\n",
    "normalized_tf_array = normalized_tf_dtm.toarray()\n",
    "\n",
    "print(\"Normalized Term Frequency DTM (L2):\\n\", normalized_tf_array)\n",
    "```\n",
    "\n",
    "### Max Normalization\n",
    "\n",
    "Max normalization scales the term frequencies by the maximum term frequency in the document. This method ensures that the term with the highest frequency in each document gets a value of 1.\n",
    "\n",
    "The formula for max normalization for a given term $ t$ in a document $ d $ is:\n",
    "\n",
    "$ \\text{Normalized TF}_{\\text{max}}(t, d) = \\frac{\\text{Frequency of } t \\text{ in } d}{\\max_{t' \\in d} (\\text{Frequency of } t' \\text{ in } d)} $\n",
    "\n",
    "### Example of Max Normalization\n",
    "\n",
    "Let's use scikit-learn's `TfidfVectorizer` with `use_idf=False` and `norm='max'` to create a DTM with max normalization.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer with max normalization\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='max')\n",
    "\n",
    "# Fit and transform the corpus to create the normalized term frequency DTM\n",
    "normalized_tf_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the DTM to an array and print it\n",
    "normalized_tf_array = normalized_tf_dtm.toarray()\n",
    "\n",
    "print(\"Normalized Term Frequency DTM (Max):\\n\", normalized_tf_array)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **L1 Normalization**: Ensures the sum of the absolute values of the vector elements is 1. Suitable for comparing documents of different lengths on a common scale.\n",
    "- **L2 Normalization**: Ensures the sum of the squares of the vector elements is 1. Often used when the magnitude of the vectors is important.\n",
    "- **Max Normalization**: Scales term frequencies by the maximum term frequency in the document. Ensures that the term with the highest frequency in each document gets a value of 1.\n",
    "\n",
    "Each normalization method has its specific use cases and should be chosen based on the particular requirements of your text processing task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2959e-18b9-4de9-93e4-1e3714021d06",
   "metadata": {},
   "source": [
    "### When to use which norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d601f-b723-43b2-8808-9c867bba1ce2",
   "metadata": {},
   "source": [
    "Sure, let's simplify:\n",
    "\n",
    "### L1 Normalization:\n",
    "- **What It Does**: It makes sure that when you add up all the absolute values of the numbers in a set, the total is 1.\n",
    "- **Use Case**: Imagine you have a bunch of numbers (like the importance of words in a document). L1 normalization makes sure that the total importance of all those words is 1. It's like dividing a cake into slices, where each slice represents the importance of one word, and all the slices together fill up the whole cake.\n",
    "- **Why You'd Use It**: If you think only a few words are super important in a document, you might want to use L1 normalization. It helps you focus on the most important stuff and ignore the rest.\n",
    "\n",
    "### L2 Normalization:\n",
    "- **What It Does**: It makes sure that when you add up all the squares of the numbers in a set, and then take the square root, the total is 1.\n",
    "- **Use Case**: Similar to L1, but it's more about making sure that all the pieces (or numbers) contribute more or less equally to the total. It's like if you have a bunch of puzzle pieces and you want to make sure no single piece is way bigger or smaller than the others when you add them up.\n",
    "- **Why You'd Use It**: If you want a more balanced approach where all features contribute somewhat equally to the final result, you might go for L2 normalization. It's good for avoiding one feature dominating the others.\n",
    "\n",
    "### Max Norm Normalization:\n",
    "- **What It Does**: It makes sure that the maximum value in a set is not bigger than a certain limit.\n",
    "- **Use Case**: Let's say you have a list of numbers, and one of them is way bigger than the others. Max norm normalization caps that big number so it doesn't throw everything out of balance.\n",
    "- **Why You'd Use It**: If you're worried about some data points being much larger than others and causing problems in your analysis or model, you might use max norm normalization. It helps keep things in check and prevents outliers from messing up your results.\n",
    "\n",
    "### Choosing Between Them:\n",
    "- **L1**: Use it if you want to focus on just a few important things and ignore the rest.\n",
    "- **L2**: Go for it if you want a more balanced approach where everything contributes somewhat equally.\n",
    "- **Max Norm**: Use it if you want to make sure no single data point dominates the others and causes issues.\n",
    "\n",
    "In short, each method helps you manage the size or importance of different pieces of data in a way that makes sense for your analysis or model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466df0a4-aa97-4c1e-8c87-517e882b5a46",
   "metadata": {},
   "source": [
    "## Objective of using TF-IDF and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aed2a2-4e57-4f1e-bf0d-dbb3d86b8ec7",
   "metadata": {},
   "source": [
    "In both TF-IDF and normalization techniques for Document-Term Matrix (DTM), the main objective is to ensure that the representation of terms in documents is not biased by their frequency alone. Let's explore why this is important:\n",
    "\n",
    "### 1. TF-IDF Method for DTM:\n",
    "In TF-IDF, the idea is to weight terms based on their importance in the corpus. This is achieved through two main components:\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often a term occurs in a document.\n",
    "- **Inverse Document Frequency (IDF)**: Measures how unique or rare a term is across all documents in the corpus.\n",
    "\n",
    "The product of TF and IDF results in a score that prioritizes terms that are frequent in a document but rare across the corpus. This is important because it helps in identifying terms that are more discriminative and descriptive of the content of a document.\n",
    "\n",
    "If a particular word has a high TF-IDF value, it means that it is important within that document but relatively rare across the entire corpus. This ensures that common words (like \"the\", \"and\", etc.) don't dominate the representation.\n",
    "\n",
    "### 2. Normalization in DTM:\n",
    "Normalization techniques, such as L2 normalization or maximum frequency normalization, are used to scale down the raw term frequencies in a document. The main objectives are:\n",
    "\n",
    "- **Fair Comparison**: By normalizing term frequencies, we make it easier to compare documents of different lengths. Otherwise, longer documents would naturally have higher raw term frequencies, which could bias comparisons.\n",
    "\n",
    "- **Reduce Noise**: In some cases, certain terms may have very high frequencies due to the nature of the document (e.g., repeated occurrences of the same word). Normalization helps to reduce the impact of such noisy terms.\n",
    "\n",
    "- **Stabilize Model**: Normalization can stabilize the model and prevent it from being overly sensitive to outliers or extreme values in term frequencies.\n",
    "\n",
    "### Conclusion:\n",
    "In both cases, the goal is to ensure that the representation of terms in the DTM is meaningful and not skewed by factors like document length or common words. By using TF-IDF and normalization techniques, we can create more robust and informative representations of textual data, which is crucial for tasks like information retrieval, text classification, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ada8a6-d349-4a3d-945d-9dc74ac237db",
   "metadata": {},
   "source": [
    "## 5. Binary DTM with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24174242-71b0-42d7-904a-73a170e4f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['and this' 'document is' 'first document' 'is the' 'second document'\n",
      " 'the first' 'the second' 'the third' 'third one' 'this document'\n",
      " 'this is']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and this</th>\n",
       "      <th>document is</th>\n",
       "      <th>first document</th>\n",
       "      <th>is the</th>\n",
       "      <th>second document</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this document</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and this  document is  first document  is the  second document  the first  \\\n",
       "0         0            0               1       1                0          1   \n",
       "1         0            1               0       1                1          0   \n",
       "2         1            0               0       1                0          0   \n",
       "\n",
       "   the second  the third  third one  this document  this is  \n",
       "0           0          0          0              0        1  \n",
       "1           1          0          0              1        0  \n",
       "2           0          1          1              0        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer with binary=True and ngram_range=(2, 2) for bigrams\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "\n",
    "# Fit and transform the corpus to create the binary with bigrams DTM\n",
    "binary_bigram_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names (bigrams)\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the DTM to an array and print it\n",
    "binary_bigram_array = pd.DataFrame(binary_bigram_dtm.toarray(),columns = bigrams)\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "binary_bigram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ab7ba7-6c80-45a5-97ce-9047cb6fb79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>first document</th>\n",
       "      <th>is</th>\n",
       "      <th>is the</th>\n",
       "      <th>the</th>\n",
       "      <th>the first</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  first  first document  is  is the  the  the first  this  this is\n",
       "0         1      1               1   1       1    1          1     1        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\"]\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "binary_ngram_dtm = vectorizer.fit_transform(corpus)\n",
    "# Get feature names (bigrams)\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the DTM to an array and print it\n",
    "binary_ngram_dtm = pd.DataFrame(binary_ngram_dtm.toarray(),columns = bigrams)\n",
    "\n",
    "binary_ngram_dtm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14b9eb-2a4f-456f-a33e-7d7e3c9c6d24",
   "metadata": {},
   "source": [
    "## 6. Frequency with N-Grams\n",
    "\n",
    "Each entry is the count of the n-gram's occurrences in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a19834-6b10-4595-ad84-9b4596f42827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [0 0 2 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0]\n",
      " [1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "freq_ngram_dtm = vectorizer.fit_transform(corpus)\n",
    "print(freq_ngram_dtm.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adfbf34-3cc7-4af7-9cbf-c001580057d6",
   "metadata": {},
   "source": [
    "## 7. TF-IDF with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b537235-765d-47e4-bd6f-2280f4667644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and this</th>\n",
       "      <th>document</th>\n",
       "      <th>document is</th>\n",
       "      <th>first</th>\n",
       "      <th>first document</th>\n",
       "      <th>is</th>\n",
       "      <th>is the</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>second document</th>\n",
       "      <th>the</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this</th>\n",
       "      <th>this document</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.250655</td>\n",
       "      <td>0.250655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250655</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515421</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  and this  document  document is     first  first document  \\\n",
       "0  0.000000  0.000000  0.322764     0.000000  0.424396        0.424396   \n",
       "1  0.000000  0.000000  0.515421     0.338858  0.000000        0.000000   \n",
       "2  0.354136  0.354136  0.000000     0.000000  0.000000        0.000000   \n",
       "\n",
       "         is    is the       one    second  second document       the  \\\n",
       "0  0.250655  0.250655  0.000000  0.000000         0.000000  0.250655   \n",
       "1  0.200135  0.200135  0.000000  0.338858         0.338858  0.200135   \n",
       "2  0.209158  0.209158  0.354136  0.000000         0.000000  0.209158   \n",
       "\n",
       "   the first  the second  the third     third  third one      this  \\\n",
       "0   0.424396    0.000000   0.000000  0.000000   0.000000  0.250655   \n",
       "1   0.000000    0.338858   0.000000  0.000000   0.000000  0.200135   \n",
       "2   0.000000    0.000000   0.354136  0.354136   0.354136  0.209158   \n",
       "\n",
       "   this document   this is  \n",
       "0       0.000000  0.322764  \n",
       "1       0.338858  0.000000  \n",
       "2       0.000000  0.269329  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_ngram_dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_ngram_dtm =  pd.DataFrame(tfidf_ngram_dtm.toarray(), columns=terms)\n",
    "tfidf_ngram_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe8e04-7ea5-4a63-b7d7-c076b62f144e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
